Copyright (c) 2020 Intel Corporation, Weqaar Janjua <weqaar.a.janjua@intel.com>

AF_XDP Veth Kernel Self-tests

# End-to-end AF_XDP over Veth test
#
# Topology:
# ---------
#                 -----------
#               _ | Process | _
#              /  -----------  \
#			  /		   |        \
#            /         |         \
#      -----------     |     -----------
#      | Thread1 |     |     | Thread2 |
#      -----------     |     -----------
#           |          |          |
#      -----------     |     -----------
#      |  xskX   |     |     |  xskY   |
#      -----------     |     -----------
#           |          |          |
#      -----------     |     ----------
#      |  vethX  | --------- |  vethY |
#      -----------   peer    ----------
#           |          |          |
#      namespaceX      |     namespaceY

AF_XDP is an address family that is optimized for high performance packet
processing, it is XDPâ€™s user-space interface.

An AF_XDP socket is linked to a single UMEM which is a region of virtual
contiguous memory, divided into equal-sized frames.

Refer to AF_XDP Kernel Documentation for detailed information:
https://www.kernel.org/doc/html/latest/networking/af_xdp.html

Tests Information:
------------------
These selftests test AF_XDP SKB and Native/DRV modes using veth
Virtual Ethernet interfaces.

Test program is a multi-threaded tx/rx process, each thread is single socket
with a unique UMEM, it validates in-order packet delivery and packet content.

The following tests are run:

1. PREREQUISITES
   Set up veth interfaces as per the topology shown ^^:
   * setup two veth interfaces and one namespace
   ** veth<xxxx> in root namespace
   ** veth<yyyy> in af_xdp<xxxx> namespace
   ** namespace af_xdp<xxxx>
   * create a spec file veth.spec that includes this run-time configuration
     that is read by test scripts - filenames prefixed with TEST_XSK
   *** xxxx and yyyy are randomnly generated 4 digit numbers used to avoid
       conflict with any existing interface.

2. AF_XDP SKB mode
   Generic mode XDP that is driver independent, used when the driver does
   not have support for XDP. Works on any netdevice using sockets and
   generic XDP path. XDP hook from netif_receive_skb().
   a. nopoll - soft-irq processing
   b. poll - busy-pollng, using poll() syscall
      No longer waiting for device interrupts being generated/handled.
	  Polling driver/device queues.
      For explanation on poll and no-poll refer to this RFC:
	  https://lore.kernel.org/bpf/
	  1556786363-28743-1-git-send-email-magnus.karlsson@intel.com/

3. AF_XDP DRV/Native mode
   Works on any netdevice with XDP support, driver dependent. Processes
   packets before SKB allocation. Provides better performance than SKB.
   Driver hook available just after DMA of buffer descriptor.
   a. nopoll
   b. poll

For all tests, currently only copy-mode is supported.

Total tests: 5.

Flow:
* Single process spawns two threads: Tx and Rx
* Each of these two threads attach to a veth interface within their assigned
  namespaces
* Each thread Creates one AF_XDP socket connected to a unique umem for each
  veth interface
* Tx thread Transmits 10k packets from vethxxxx to vethyyyy
* Rx thread verifies if all 10k packets were received and delivered in-order

Kernel configuration:
---------------------
See "config" file for recommended kernel config options.

Turn on XDP sockets and veth support when compiling i.e.
	Networking support -->
		Networking options -->
			[ * ] XDP sockets

Executing Tests:
----------------
Must run as "root".

Run (summary only):
  sudo make summary=1 run_tests

Run (full output):
  sudo make run_tests

Clean:
  make clean
